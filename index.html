<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Compositionality and Reasoning in AI and Cognitive Science — Workshop</title>
  <meta name="description" content="One-day workshop on Compositionality and Reasoning in AI and Cognitive Science — January 8, 2026 — Warsaw">

  <!-- Google fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">

  <style>
    :root{
      --accent:#0b63d4;
      --bg:#f7f9fc;
      --text:#17202a;
      --muted:#51657a;
      --max-width:980px;
    }
    *{box-sizing:border-box}
    body{font-family:Inter,system-ui,Segoe UI,Roboto,Arial,sans-serif;margin:0;color:var(--text);background:var(--bg);line-height:1.5}
    a{color:var(--accent);text-decoration:none}

    .container{max-width:var(--max-width);margin:0 auto;padding:28px}

    /* Navigation */
    nav{
      display:flex;
      justify-content:space-between;
      align-items:center;
      padding:12px 0;
      background:white;
      position:sticky;
      top:0;
      z-index:1000;
      border-bottom:1px solid #ddd;
    }
    .nav-links{display:flex;gap:14px;flex-wrap:wrap}

    /* Hero */
    .hero{
      position:relative;
      height:320px;
      display:flex;
      align-items:flex-end;
      border-radius:8px;
      overflow:hidden;
      background:#222;
      margin-bottom:24px;
    }
    .hero img{
      position:absolute;
      inset:0;
      width:100%;
      height:100%;
      object-fit:cover;
      filter:contrast(1.05) saturate(0.95);
    }
    .hero-overlay{
      position:relative;
      background:linear-gradient(180deg,rgba(0,0,0,0.10),rgba(0,0,0,0.45));
      padding:28px;
      color:white;
      width:100%;
    }
    .site-title{
      margin:0;
      font-size:clamp(20px,3.2vw,36px);
      line-height:1.05;
    }
    .site-sub{
      margin-top:6px;
      color:rgba(255,255,255,0.9);
      font-weight:400;
    }

    /* Sections */
    section{
      background:white;
      margin-top:18px;
      padding:28px;
      border-radius:8px;
      box-shadow:0 6px 18px rgba(19,30,43,0.04);
    }
    h2{margin-top:0}
    .muted{color:var(--muted)}
    ul{padding-left:1.1em}
    .speaker{margin-bottom:10px}

    footer {
      margin-top: 24px;
      text-align: center;
      padding: 20px;
      color: var(--muted);
      font-size: 13px;
      background: white;
      border-radius: 8px;
      box-shadow: 0 6px 18px rgba(19, 30, 43, 0.04);
    }

    /* Logo container styling */
    .logo-container {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 40px;
      margin-bottom: 10px;
      flex-wrap: wrap;
    }

    /* Individual logos */
    .logo-container img {
      height: 60px;
      width: auto;
      object-fit: contain;
      transition: transform 0.2s ease-in-out;
    }

    /* Optional hover effect */
    .logo-container img:hover {
      transform: scale(1.05);
    }

    :root{
      --accent:#0b63d4;
      --bg:#f7f9fc;
      --text:#17202a;
      --muted:#51657a;
      --max-width:980px;
    }
    *{box-sizing:border-box}
    body{font-family:Inter,system-ui,Segoe UI,Roboto,Arial,sans-serif;margin:0;color:var(--text);background:var(--bg);line-height:1.5}
    a{color:var(--accent);text-decoration:none}

    html {
      scroll-behavior: smooth; /* Smooth scrolling */
    }

    .container{max-width:var(--max-width);margin:0 auto;padding:28px}

    section {
      scroll-margin-top: 70px; /* Fix offset when clicking links */
    }
  </style>
</head>
<body>
  <div class="container">
    <nav>
      <div style="font-weight:700">Compositionality &amp; Reasoning — Workshop</div>
      <div class="nav-links muted">
        <a href="#about">About</a>
        <a href="#papers">Papers</a>
        <a href="#program">Program</a>
        <a href="#speakers">Speakers</a>
        <a href="#venue">Venue</a>
        <a href="#contact">Contact</a>
      </div>
    </nav>

    <!-- HERO -->
    <header class="hero" role="banner">
      <img src="images/header.jpg" alt="Workshop header image">
      <div class="hero-overlay">
        <h1 class="site-title">Compositionality and Reasoning in AI and Cognitive Science</h1>
        <div class="site-sub">Warsaw, January 8, 2026</div>
      </div>
    </header>

    <!-- About -->
    <section id="about">
      <h2>About the Workshop</h2>
      <p class="muted">
        <div style="text-align: justify;">
        In neural models, generalization is the ability to apply learned knowledge to new, unseen data. Compositionality is a principle that enables such generalization by allowing complex structures to be represented and processed as combinations of simpler elements, which provides a systematic way to interpret new structures. Together, they are a key factor in bridging the gap between learning and genuine adaptability in AI.<br>        
        This one‑day workshop aims to discuss compositionality, structured representation, and the integration of neural and symbolic reasoning in modern learning systems.
        </div>
      </p>

      <h3>Our research:</h3>
      <p class="muted">
          <div style="text-align: justify;">
          Our work investigates hybrid models of reasoning, a neuro-symbolic approach to deductive inference that integrates neural learning with symbolic logic within restricted fragments of natural language. We began with a pilot study [1] on a simple propositional corpus to examine whether neural networks can assist a symbolic prover by selecting necessary formulas from a knowledge base to prove a given hypothesis. We then extended this work [2] to the syllogistic fragment, evaluating feedforward, recurrent, convolutional, and transformer architectures. Despite the simplicity of the experimental setup—training and testing on a single knowledge base with one-hot encoded inputs—our results indicated that models trained from scratch failed to capture the underlying logical structure. To validate and extend these findings, we next employed modern pretrained language models for formula selection in direct and indirect proofs, integrating neural assistants with a symbolic prover to evaluate their interaction within a hybrid reasoning framework [3]. In this phase, models were trained on multiple knowledge bases and tested on unseen ones, using pseudoword-based textual representations for both input and output.
          To further investigate the compositionality limitation in neural models, and in collaboration with researchers from the University of Trento, we explored a meta-learning approach on our syllogistic corpus [4] to study how models adapt to novel reasoning patterns in the formula selection task.<br><br>
          Compositionality is a fundamental component of reasoning and continues to challenge neural models. Our results suggest that hybrid architectures integrating symbolic inference with neural learning offer a promising path toward overcoming these limitations and, more broadly, a compelling direction for future exploration—shedding light on how structured reasoning can emerge from pattern-based learning systems in AI.
          </div>
      </p>
    </section>

    <!-- Published Papers -->
    <section id="papers">
      <h2>Published Papers</h2>
      <ul style="list-style:none; padding-left:0;">
        <li style="margin-bottom:20px;">
          <strong><a href="https://hal.science/hal-03846838v1/document" target="_blank">[1] Compositionality in a simple corpus</a></strong>
          <details style="margin-top:6px;">
            <summary style="cursor:pointer; color:var(--accent);">Show Abstract</summary>
            <p class="muted" style="margin-top:6px; text-align: justify;">
              We investigate the capacity of neural networks (NNs) to learn compositional structures by focusing on a well-defined simple logical corpus, and on proof-centered compositionality. We conduct our investigation in a minimal setting by creating a simple logical corpus, where all compositionality-related phenomena come from the structure of proofs as all the sentences of the corpus are propositional logic implications. By training NNs on this corpus we test different aspects of compositionality, through variations of proof lengths and permutations of the constants.
            </p>
          </details>
        </li>

        <li style="margin-bottom:20px;">
          <strong><a href="https://aclanthology.org/2024.findings-naacl.147.pdf" target="_blank">[2] Testing the limits of logical reasoning in neural and hybrid models</a></strong>
          <details style="margin-top:6px;">
            <summary style="cursor:pointer; color:var(--accent);">Show Abstract</summary>
            <p class="muted" style="margin-top:6px; text-align: justify;">
              We study the ability of neural and hybrid models to generalize logical reasoning patterns. We created a series of tests for analyzing various aspects of generalization in the context of language and reasoning, focusing on compositionality and recursiveness. We used them to study the syllogistic logic in hybrid models, where the network assists in premise selection. We analyzed feed-forward, recurrent, convolutional, and transformer architectures. Our experiments demonstrate that even though the models can capture elementary aspects of the meaning of logical terms, they learn to generalize logical reasoning only to a limited degree.
            </p>
          </details>
        </li>

        <li style="margin-bottom:20px;">
          <strong><a href="https://arxiv.org/pdf/2510.09472" target="_blank">[3] Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic</a></strong>
          <details style="margin-top:6px;">
            <summary style="cursor:pointer; color:var(--accent);">Show Abstract</summary>
            <p class="muted" style="margin-top:6px; text-align: justify;">
              Despite the remarkable progress in neural models, their ability to generalize—a cornerstone for applications like logical reasoning—remains a critical challenge. We delineate two fundamental aspects of this ability: compositionality, the capacity to abstract atomic logical rules underlying complex inferences, and recursiveness, the aptitude to build intricate representations through iterative application of inference rules. In the literature, these two aspects are often confounded together under the umbrella term of generalization. To sharpen this distinction, we investigated the logical generalization capabilities of pre-trained large language models (LLMs) using the syllogistic fragment as a benchmark for natural language reasoning. Though simple, this fragment provides a foundational yet expressive subset of formal logic that supports controlled evaluation of essential reasoning abilities. Our findings reveal a significant disparity: while LLMs demonstrate reasonable proficiency in recursiveness, they struggle with compositionality. To overcome these limitations and establish a reliable logical prover, we propose a hybrid architecture integrating symbolic reasoning with neural computation. This synergistic interaction enables robust and efficient inference—neural components accelerate processing, while symbolic reasoning ensures completeness. Our experiments show that high efficiency is preserved even with relatively small neural components. As part of our proposed methodology, this analysis gives a rationale and highlights the potential of hybrid models to effectively address key generalization barriers in neural reasoning systems.
            </p>
          </details>
        </li>

        <li style="margin-bottom:20px;">
          <strong><a href="https://arxiv.org/pdf/2505.14313" target="_blank">[4] Teaching Small Language Models to Learn Logic through Meta-Learning</a></strong>
          <details style="margin-top:6px;">
            <summary style="cursor:pointer; color:var(--accent);">Show Abstract</summary>
            <p class="muted" style="margin-top:6px; text-align: justify;">
              Large language models (LLMs) are increasingly evaluated on reasoning tasks, yet their logical abilities remain contested. To address this, we study LLMs' reasoning in a well-defined fragment of logic: syllogistic reasoning. We cast the problem as premise selection and construct controlled datasets to isolate logical competence. Beyond evaluation, an open challenge is enabling LLMs to acquire abstract inference patterns that generalize to novel structures. We propose to apply few-shot meta-learning to this domain, thereby encouraging models to extract rules across tasks rather than memorize patterns within tasks. Although meta-learning has been little explored in the context of logic learnability, our experiments show that it is effective: small models (1.5B-7B) fine-tuned with meta-learning demonstrate strong gains in generalization, with especially pronounced benefits in low-data regimes. These meta-learned models outperform GPT-4o and o3-mini on our syllogistic reasoning task. 
            </p>
          </details>
        </li>

      </ul>
    </section>

    <!-- Program -->
    <section id="program">
      <h2>Program</h2>
      <ul>
        <li><strong>9:00–12:00</strong> — Talks & presentations (part 1)</li>
        <li><strong>12:00–14:00</strong> — Lunch break</li>
        <li><strong>14:00–17:00</strong> — Talks & presentations (part 2)</li>
      </ul>
    </section>

    <!-- Speakers -->
    <section id="speakers">
      <h2>Invited Speakers</h2>

      <div class="speaker">
        <strong><a href="https://dieuwkehupkes.nl/" target="_blank" rel="noopener">Dieuwke Hupkes</a></strong>
        <div class="muted">Meta AI Research — Title</div>
      </div>
      
      <div class="speaker">
        <strong><a href="https://personalpages.manchester.ac.uk/staff/ian.pratt/" target="_blank" rel="noopener">Ian Pratt-Hartmann</a></strong>
      <div class="muted">University of Manchester — Title</div></div>      

    </section>

    <!-- Venue -->
    <section id="venue">
      <h2>Venue</h2>
      <p class="muted">Venue: <strong>TBD — Warsaw</strong><br>
      Date: <strong>January 8, 2026</strong></p>
      <p class="muted">How to reach the venue.</p>
    </section>

    <!-- Contact -->
    <section id="contact">
      <h2>Contact</h2>
      <p class="muted">
        For questions about the workshop please contact:
      </p>
      <ul class="muted">
        <li><a href="mailto:mmalicki@mimuw.edu.pl">Maciej Malicki</a> (University of Warsaw)</li>
        <li><a href="mailto:jakub.szymanik@unitn.it">Jakub Szymanik</a> (University of Trento)</li>
      </ul>
    </section>

    <!-- Footer Section -->
    <footer>
      <div class="logo-container">
        <a href="https://www.uw.edu.pl/" target="_blank" rel="noopener">
          <img src="images/uw_logo.png" alt="University of Warsaw Logo">
        </a>
        <a href="https://www.unitn.it/" target="_blank" rel="noopener">
          <img src="images/ut_logo.jpg" alt="University of Trento Logo">
        </a>
      </div>
      <p>&copy; 2025 Compositionality &amp; Reasoning Workshop — University of Warsaw.</p>
    </footer>
  </div>
</body>
</html>
